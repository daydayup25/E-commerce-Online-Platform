{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbffd7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENHANCED PIPELINE: MODEL SELECTION -> RECOMMENDATION -> SIMULATION\n",
      "======================================================================\n",
      "\n",
      "LOADING DATA...\n",
      "Final dataset: 7,795 records\n",
      "\n",
      "======================================================================\n",
      "PHASE 1: MODEL SELECTION COMPARISON\n",
      "======================================================================\n",
      "Model Comparison Results:\n",
      "--------------------------------------------------\n",
      "LogisticRegression  : Accuracy = 0.8185, AUC = 0.6282\n",
      "DecisionTree        : Accuracy = 0.7940, AUC = 0.5473\n",
      "RandomForest        : Accuracy = 0.8230, AUC = 0.6512\n",
      "GradientBoosting    : Accuracy = 0.8247, AUC = 0.6238\n",
      "AdaBoost            : Accuracy = 0.8169, AUC = 0.5946\n",
      "LinearDiscriminant  : Accuracy = 0.8169, AUC = 0.6274\n",
      "KNN                 : Accuracy = 0.8029, AUC = 0.6292\n",
      "\n",
      "âœ… Best Model Selected: GradientBoosting\n",
      "   Test Accuracy: 0.8247\n",
      "   Test AUC: 0.6238\n",
      "\n",
      "ðŸ“Š Feature Importance (Top 5):\n",
      "   delivery_days       : 0.5141\n",
      "   freight_value       : 0.2119\n",
      "   price               : 0.1316\n",
      "   purchase_month      : 0.0565\n",
      "   purchase_hour       : 0.0503\n",
      "\n",
      "GLOBAL SPLIT: Train=6,236, Test=1,559\n",
      "\n",
      "======================================================================\n",
      "PHASE 2: RECOMMENDATION WITH BEST MODEL\n",
      "======================================================================\n",
      "Tuning SVD components...\n",
      "  k=5: MAE = 2.3473\n",
      "  k=10: MAE = 1.9654\n",
      "  k=20: MAE = 1.7091\n",
      "  k=30: MAE = 1.6140\n",
      "âœ“ Best SVD: k=30, MAE=1.6140\n",
      "\n",
      "======================================================================\n",
      "PHASE 3: SIMULATION WITH OPTIMIZED PIPELINE\n",
      "======================================================================\n",
      "\n",
      "FINAL SIMULATION ACCURACY: 0.8230\n",
      "Actions Distribution:\n",
      "action\n",
      "UPSELL                  856\n",
      "UPSELL (New Profile)    100\n",
      "RETENTION                44\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ“ˆ NEW FEATURES ANALYSIS:\n",
      "Average Category Diversity by Action:\n",
      "action\n",
      "RETENTION               1.090909\n",
      "UPSELL                  1.028037\n",
      "UPSELL (New Profile)    1.000000\n",
      "Name: category_diversity, dtype: float64\n",
      "\n",
      "Weekend Purchase Ratio by Action:\n",
      "action\n",
      "RETENTION               0.227273\n",
      "UPSELL                  0.233645\n",
      "UPSELL (New Profile)    0.290000\n",
      "Name: is_weekend, dtype: float64\n",
      "\n",
      "ðŸ’¾ Results saved to 'enhanced_customer_retention.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "base_path = \"/Users/tilincet/Desktop/INDENG242A/project_Brazil/archive-3\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENHANCED PIPELINE: MODEL SELECTION -> RECOMMENDATION -> SIMULATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD AND PRE-FILTER DATA\n",
    "# ==========================================\n",
    "print(\"\\nLOADING DATA...\")\n",
    "files = {\n",
    "    'customers': 'olist_customers_dataset.csv',\n",
    "    'orders': 'olist_orders_dataset.csv', \n",
    "    'order_items': 'olist_order_items_dataset.csv',\n",
    "    'order_payments': 'olist_order_payments_dataset.csv',\n",
    "    'order_reviews': 'olist_order_reviews_dataset.csv',\n",
    "    'products': 'olist_products_dataset.csv',\n",
    "    'translation': 'product_category_name_translation.csv'\n",
    "}\n",
    "\n",
    "data = {}\n",
    "for key, file in files.items():\n",
    "    data[key] = pd.read_csv(f\"{base_path}/{file}\")\n",
    "    \n",
    "df_orders = data['orders'].merge(\n",
    "    data['customers'][['customer_id', 'customer_unique_id']], on='customer_id', how='inner'\n",
    ")\n",
    "\n",
    "cust_counts = df_orders['customer_unique_id'].value_counts()\n",
    "repeat_customers = cust_counts[cust_counts > 1].index\n",
    "df_orders = df_orders[df_orders['customer_unique_id'].isin(repeat_customers)]\n",
    "\n",
    "full_df = df_orders.merge(data['order_reviews'][['order_id', 'review_score']], on='order_id', how='inner')\n",
    "full_df = full_df.merge(data['order_items'][['order_id', 'product_id', 'price', 'freight_value']], on='order_id', how='inner')\n",
    "full_df = full_df.merge(data['products'][['product_id', 'product_category_name']], on='product_id', how='left')\n",
    "full_df = full_df.merge(data['translation'], on='product_category_name', how='left')\n",
    "full_df['order_purchase_timestamp'] = pd.to_datetime(full_df['order_purchase_timestamp'])\n",
    "full_df['order_delivered_customer_date'] = pd.to_datetime(full_df['order_delivered_customer_date'], errors='coerce')\n",
    "full_df['delivery_days'] = (full_df['order_delivered_customer_date'] - full_df['order_purchase_timestamp']).dt.days\n",
    "\n",
    "# NEW FEATURE 1\n",
    "category_count = full_df.groupby('order_id')['product_category_name_english'].nunique().reset_index()\n",
    "category_count.columns = ['order_id', 'category_diversity']\n",
    "full_df = full_df.merge(category_count, on='order_id', how='left')\n",
    "\n",
    "# NEW FEATURE 2\n",
    "full_df['purchase_hour'] = full_df['order_purchase_timestamp'].dt.hour\n",
    "full_df['is_weekend'] = (full_df['order_purchase_timestamp'].dt.dayofweek >= 5).astype(int)\n",
    "full_df['purchase_month'] = full_df['order_purchase_timestamp'].dt.month\n",
    "\n",
    "full_df['high_satisfaction'] = (full_df['review_score'] >= 4).astype(int)\n",
    "\n",
    "full_df = full_df.dropna(subset=['delivery_days', 'product_category_name_english', 'category_diversity'])\n",
    "\n",
    "print(f\"Final dataset: {len(full_df):,} records\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL SELECTION PHASE\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 1: MODEL SELECTION COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def prepare_model_selection_data(df):\n",
    "    order_agg = df.groupby('order_id').agg({\n",
    "        'price': 'sum',\n",
    "        'freight_value': 'sum', \n",
    "        'delivery_days': 'max',\n",
    "        'category_diversity': 'max',    \n",
    "        'purchase_hour': 'first',\n",
    "        'is_weekend': 'first',          \n",
    "        'purchase_month': 'first',      \n",
    "        'high_satisfaction': 'max'\n",
    "    }).reset_index()\n",
    "\n",
    "    X = order_agg[['price', 'freight_value', 'delivery_days', \n",
    "                   'category_diversity', 'purchase_hour', 'is_weekend', 'purchase_month']]\n",
    "    y = order_agg['high_satisfaction']\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def compare_classification_models(X, y):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    models = {\n",
    "        'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'DecisionTree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "        'LinearDiscriminant': LinearDiscriminantAnalysis(),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    print(\"Model Comparison Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            if name in ['LogisticRegression', 'KNN']:\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_proba)\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'scaler': scaler if name in ['LogisticRegression', 'KNN'] else None\n",
    "            }\n",
    "            \n",
    "            print(f\"{name:<20}: Accuracy = {accuracy:.4f}, AUC = {auc:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results, X_train, X_test, y_train, y_test\n",
    "\n",
    "X, y = prepare_model_selection_data(full_df)\n",
    "model_results, X_train, X_test, y_train, y_test = compare_classification_models(X, y)\n",
    "\n",
    "if model_results:\n",
    "    best_model_name = max(model_results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "    best_model = model_results[best_model_name]['model']\n",
    "    best_scaler = model_results[best_model_name]['scaler']\n",
    "    \n",
    "    print(f\"\\nâœ… Best Model Selected: {best_model_name}\")\n",
    "    print(f\"   Test Accuracy: {model_results[best_model_name]['accuracy']:.4f}\")\n",
    "    print(f\"   Test AUC: {model_results[best_model_name]['auc']:.4f}\")\n",
    "\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        print(f\"\\nðŸ“Š Feature Importance (Top 5):\")\n",
    "        for i, row in feature_importance.head().iterrows():\n",
    "            print(f\"   {row['feature']:<20}: {row['importance']:.4f}\")\n",
    "else:\n",
    "    print(\"âŒ No models were successfully trained\")\n",
    "    exit()\n",
    "\n",
    "# ==========================================\n",
    "# 3. GLOBAL SPLIT (80/20)\n",
    "# ==========================================\n",
    "global_train_df, global_test_df = train_test_split(full_df, test_size=0.2, random_state=42)\n",
    "print(f\"\\nGLOBAL SPLIT: Train={len(global_train_df):,}, Test={len(global_test_df):,}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. STAGE 2: RECOMMENDER WITH BEST MODEL\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 2: RECOMMENDATION WITH BEST MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ratings_long = (\n",
    "    global_train_df[['customer_unique_id', 'product_category_name_english', 'review_score']]\n",
    "    .dropna()\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "ratings_long.rename(columns={\n",
    "    'customer_unique_id': 'user',\n",
    "    'product_category_name_english': 'item', \n",
    "    'review_score': 'rating'\n",
    "}, inplace=True)\n",
    "\n",
    "users = ratings_long['user'].unique()\n",
    "items = ratings_long['item'].unique()\n",
    "user2idx = {u: i for i, u in enumerate(users)}\n",
    "item2idx = {m: j for j, m in enumerate(items)}\n",
    "idx2item = {j: m for m, j in item2idx.items()}\n",
    "\n",
    "n_users, n_items = len(users), len(items)\n",
    "\n",
    "ratings_long['u_idx'] = ratings_long['user'].map(user2idx)\n",
    "ratings_long['i_idx'] = ratings_long['item'].map(item2idx)\n",
    "\n",
    "inner_train, inner_val = train_test_split(ratings_long, test_size=0.2, random_state=42)\n",
    "\n",
    "user_bias = inner_train.groupby('u_idx')['rating'].mean().reindex(range(n_users)).fillna(0).values\n",
    "item_bias = inner_train.groupby('i_idx')['rating'].mean().reindex(range(n_items)).fillna(0).values\n",
    "\n",
    "R_centered = np.zeros((n_users, n_items), dtype=float)\n",
    "for row in inner_train.itertuples(index=False):\n",
    "    u, i = int(row.u_idx), int(row.i_idx)\n",
    "    R_centered[u, i] = row.rating - user_bias[u] - item_bias[i]\n",
    "\n",
    "def tune_svd(k_list, R_centered, val_df, user_bias, item_bias):\n",
    "    best_k, best_mae, best_svd, best_Z_hat = None, float('inf'), None, None\n",
    "    \n",
    "    for k in k_list:\n",
    "        svd = TruncatedSVD(n_components=k, random_state=42)\n",
    "        U = svd.fit_transform(R_centered)\n",
    "        Z_hat = U @ svd.components_\n",
    "        \n",
    "        val_preds, val_actuals = [], []\n",
    "        for row in val_df.itertuples(index=False):\n",
    "            u, i = int(row.u_idx), int(row.i_idx)\n",
    "            if u < Z_hat.shape[0] and i < Z_hat.shape[1]:\n",
    "                pred = user_bias[u] + item_bias[i] + Z_hat[u, i]\n",
    "                val_preds.append(pred)\n",
    "                val_actuals.append(row.rating)\n",
    "        \n",
    "        mae = mean_absolute_error(val_actuals, val_preds) if val_preds else float('inf')\n",
    "        print(f\"  k={k}: MAE = {mae:.4f}\")\n",
    "        \n",
    "        if mae < best_mae:\n",
    "            best_mae, best_k, best_svd, best_Z_hat = mae, k, svd, Z_hat\n",
    "    \n",
    "    return best_k, best_mae, best_svd, best_Z_hat\n",
    "\n",
    "print(\"Tuning SVD components...\")\n",
    "k_list = [5, 10, 20, 30]\n",
    "best_k, best_mae, best_svd, best_Z_hat = tune_svd(k_list, R_centered, inner_val, user_bias, item_bias)\n",
    "print(f\"âœ“ Best SVD: k={best_k}, MAE={best_mae:.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. FINAL SIMULATION WITH BEST MODEL\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 3: SIMULATION WITH OPTIMIZED PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "test_orders = global_test_df.groupby('order_id')\n",
    "\n",
    "for i, (order_id, group) in enumerate(test_orders):\n",
    "    if i >= 1000: \n",
    "        break\n",
    "\n",
    "    price_total = group['price'].sum()\n",
    "    freight_total = group['freight_value'].sum() \n",
    "    days_actual = group['delivery_days'].max()\n",
    "    customer_unique_id = group['customer_unique_id'].iloc[0]\n",
    "    actual_satisfaction = group['high_satisfaction'].max()\n",
    "    \n",
    "    category_diversity = group['category_diversity'].iloc[0]\n",
    "    purchase_hour = group['purchase_hour'].iloc[0]\n",
    "    is_weekend = group['is_weekend'].iloc[0]\n",
    "    purchase_month = group['purchase_month'].iloc[0]\n",
    "\n",
    "    features = pd.DataFrame([[price_total, freight_total, days_actual, \n",
    "                            category_diversity, purchase_hour, is_weekend, purchase_month]], \n",
    "                          columns=['price', 'freight_value', 'delivery_days', \n",
    "                                 'category_diversity', 'purchase_hour', 'is_weekend', 'purchase_month'])\n",
    "    \n",
    "    if best_scaler is not None:\n",
    "        features_scaled = best_scaler.transform(features)\n",
    "        pred_satisfied = best_model.predict(features_scaled)[0]\n",
    "    else:\n",
    "        pred_satisfied = best_model.predict(features)[0]\n",
    "    \n",
    "    decision, recs = \"\", []\n",
    "    \n",
    "    if pred_satisfied == 1:\n",
    "        decision = \"UPSELL\"\n",
    "        if customer_unique_id in user2idx:\n",
    "            u_idx = user2idx[customer_unique_id]\n",
    "            scores = user_bias[u_idx] + item_bias + best_Z_hat[u_idx, :]\n",
    "            top_indices = np.argsort(scores)[::-1][:3]\n",
    "            recs = [idx2item[i] for i in top_indices if i in idx2item]\n",
    "        else:\n",
    "            decision = \"UPSELL (New Profile)\"\n",
    "            recs = [\"Trending Items\"]\n",
    "    else:\n",
    "        decision = \"RETENTION\" \n",
    "        recs = [\"Discount Coupon\"]\n",
    "    \n",
    "    results.append({\n",
    "        'order_id': order_id,\n",
    "        'customer_id': customer_unique_id,\n",
    "        'actual': actual_satisfaction,\n",
    "        'predicted': pred_satisfied,\n",
    "        'action': decision,\n",
    "        'recommendations': recs,\n",
    "        'model_used': best_model_name,\n",
    "        'category_diversity': category_diversity,\n",
    "        'purchase_hour': purchase_hour,\n",
    "        'is_weekend': is_weekend,\n",
    "        'purchase_month': purchase_month\n",
    "    })\n",
    "\n",
    "# ==========================================\n",
    "# 6. RESULTS AND EVALUATION\n",
    "# ==========================================\n",
    "res_df = pd.DataFrame(results)\n",
    "accuracy = accuracy_score(res_df['actual'], res_df['predicted'])\n",
    "print(f\"\\nFINAL SIMULATION ACCURACY: {accuracy:.4f}\")\n",
    "print(f\"Actions Distribution:\")\n",
    "print(res_df['action'].value_counts())\n",
    "\n",
    "print(f\"\\nðŸ“ˆ NEW FEATURES ANALYSIS:\")\n",
    "print(f\"Average Category Diversity by Action:\")\n",
    "print(res_df.groupby('action')['category_diversity'].mean())\n",
    "print(f\"\\nWeekend Purchase Ratio by Action:\")\n",
    "weekend_ratio = res_df.groupby('action')['is_weekend'].mean()\n",
    "print(weekend_ratio)\n",
    "\n",
    "res_df.to_csv('enhanced_customer_retention.csv', index=False)\n",
    "print(f\"\\nðŸ’¾ Results saved to 'enhanced_customer_retention.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d430b16-f27a-4e55-9067-5f9d0a9fb1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PIPELINE: REPEAT CUSTOMERS ONLY -> TRAIN -> SIMULATE\n",
      "======================================================================\n",
      "\n",
      "LOADING DATA...\n",
      "\n",
      "FILTERING FOR REPEAT BUYERS...\n",
      "  Original Orders: 99,441\n",
      "  Filtered Orders: 6,342 (Belonging to users with 2+ orders)\n",
      "  Unique Repeat Users: 2,997\n",
      "Total records after merge & cleanup: 7,795\n",
      "\n",
      "GLOBAL SPLIT:\n",
      "âœ“ Training Set: 6,236\n",
      "âœ“ Test Set:     1,559\n",
      "\n",
      "[Stage 1] Training Satisfaction Model...\n",
      "âœ“ Classifier Trained\n",
      "\n",
      "[Stage 2] Training SVD Recommender on REPEAT USERS...\n",
      "Constructing Centered Matrix...\n",
      "Tuning k over: [5, 10, 20, 30]\n",
      "  k=5: MAE = 2.3473\n",
      "  k=10: MAE = 1.9654\n",
      "  k=20: MAE = 1.7091\n",
      "  k=30: MAE = 1.6140\n",
      "âœ“ Best model: k=30\n",
      "\n",
      "======================================================================\n",
      "RUNNING SIMULATION ON REPEAT CUSTOMERS TEST SET\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "base_path = \".\"   # Update to your data path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PIPELINE: REPEAT CUSTOMERS ONLY -> TRAIN -> SIMULATE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD AND PRE-FILTER DATA\n",
    "# ==========================================\n",
    "print(\"\\nLOADING DATA...\")\n",
    "files = {\n",
    "    'customers': 'olist_customers_dataset.csv',\n",
    "    'orders': 'olist_orders_dataset.csv',\n",
    "    'order_items': 'olist_order_items_dataset.csv',\n",
    "    'order_reviews': 'olist_order_reviews_dataset.csv',\n",
    "    'products': 'olist_products_dataset.csv',\n",
    "    'translation': 'product_category_name_translation.csv'\n",
    "}\n",
    "\n",
    "data = {}\n",
    "for key, file in files.items():\n",
    "    data[key] = pd.read_csv(f\"{base_path}/{file}\")\n",
    "\n",
    "# --- CRITICAL STEP: MAP TO UNIQUE CUSTOMER ID ---\n",
    "# In Olist, 'customer_id' changes every order. 'customer_unique_id' is the real user.\n",
    "df_orders = data['orders'].merge(\n",
    "    data['customers'][['customer_id', 'customer_unique_id']], \n",
    "    on='customer_id', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# --- FILTER: KEEP ONLY REPEAT CUSTOMERS ---\n",
    "print(\"\\nFILTERING FOR REPEAT BUYERS...\")\n",
    "cust_counts = df_orders['customer_unique_id'].value_counts()\n",
    "repeat_customers = cust_counts[cust_counts > 1].index\n",
    "\n",
    "initial_count = len(df_orders)\n",
    "df_orders = df_orders[df_orders['customer_unique_id'].isin(repeat_customers)]\n",
    "filtered_count = len(df_orders)\n",
    "\n",
    "print(f\"  Original Orders: {initial_count:,}\")\n",
    "print(f\"  Filtered Orders: {filtered_count:,} (Belonging to users with 2+ orders)\")\n",
    "print(f\"  Unique Repeat Users: {df_orders['customer_unique_id'].nunique():,}\")\n",
    "\n",
    "# Merge remaining datasets (Items, Reviews, Products)\n",
    "full_df = df_orders.merge(\n",
    "    data['order_reviews'][['order_id', 'review_score']], \n",
    "    on='order_id', how='inner'\n",
    ")\n",
    "full_df = full_df.merge(\n",
    "    data['order_items'][['order_id', 'product_id', 'price', 'freight_value']], \n",
    "    on='order_id', how='inner'\n",
    ")\n",
    "full_df = full_df.merge(\n",
    "    data['products'][['product_id', 'product_category_name']], \n",
    "    on='product_id', how='left'\n",
    ")\n",
    "full_df = full_df.merge(\n",
    "    data['translation'], \n",
    "    on='product_category_name', how='left'\n",
    ")\n",
    "\n",
    "# Feature Engineering ã€revisionã€‘\n",
    "full_df['order_purchase_timestamp'] = pd.to_datetime(full_df['order_purchase_timestamp'])\n",
    "full_df['order_delivered_customer_date'] = pd.to_datetime(\n",
    "    full_df['order_delivered_customer_date'], errors='coerce'\n",
    ")\n",
    "full_df['delivery_days'] = (\n",
    "    full_df['order_delivered_customer_date'] - full_df['order_purchase_timestamp']\n",
    ").dt.days\n",
    "\n",
    "# Cleanup\n",
    "full_df = full_df.dropna(subset=['delivery_days', 'product_category_name_english'])\n",
    "full_df['high_satisfaction'] = (full_df['review_score'] >= 4).astype(int)\n",
    "\n",
    "print(f\"Total records after merge & cleanup: {len(full_df):,}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. GLOBAL SPLIT (80/20)\n",
    "# ==========================================\n",
    "# We split by ORDER, but since everyone has >1 order, \n",
    "# it is highly likely a user appears in both sets.\n",
    "global_train_df, global_test_df = train_test_split(full_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nGLOBAL SPLIT:\")\n",
    "print(f\"âœ“ Training Set: {len(global_train_df):,}\")\n",
    "print(f\"âœ“ Test Set:     {len(global_test_df):,}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. STAGE 1: SATISFACTION MODEL\n",
    "# ==========================================\n",
    "print(\"\\n[Stage 1] Training Satisfaction Model...\")\n",
    "\n",
    "# Aggregate by order for classification features\n",
    "train_agg = global_train_df.groupby('order_id').agg({\n",
    "    'price': 'sum',\n",
    "    'freight_value': 'sum',\n",
    "    'delivery_days': 'max',\n",
    "    'high_satisfaction': 'max'\n",
    "}).reset_index()\n",
    "\n",
    "X_train_sat = train_agg[['price', 'freight_value', 'delivery_days']]\n",
    "y_train_sat = train_agg['high_satisfaction']\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)\n",
    "clf.fit(X_train_sat, y_train_sat)\n",
    "print(\"âœ“ Classifier Trained\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. STAGE 2: RECOMMENDER (TUNING K)\n",
    "# ==========================================\n",
    "print(\"\\n[Stage 2] Training SVD Recommender on REPEAT USERS...\")\n",
    "\n",
    "# Use 'customer_unique_id' as 'user' now!\n",
    "ratings_long = (\n",
    "    global_train_df[['customer_unique_id', 'product_category_name_english', 'review_score']]\n",
    "    .dropna()\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "ratings_long.rename(columns={\n",
    "    'customer_unique_id': 'user',  # CHANGED FROM customer_id\n",
    "    'product_category_name_english': 'item',\n",
    "    'review_score': 'rating'\n",
    "}, inplace=True)\n",
    "\n",
    "# Map Indices\n",
    "users = ratings_long['user'].unique()\n",
    "items = ratings_long['item'].unique()\n",
    "\n",
    "user2idx = {u: i for i, u in enumerate(users)}\n",
    "item2idx = {m: j for j, m in enumerate(items)}\n",
    "idx2item = {j: m for m, j in item2idx.items()}\n",
    "\n",
    "n_users = len(users)\n",
    "n_items = len(items)\n",
    "\n",
    "ratings_long['u_idx'] = ratings_long['user'].map(user2idx)\n",
    "ratings_long['i_idx'] = ratings_long['item'].map(item2idx)\n",
    "\n",
    "# Inner Split for Tuning\n",
    "inner_train, inner_val = train_test_split(ratings_long, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate Biases\n",
    "user_bias_s = inner_train.groupby('u_idx')['rating'].mean()\n",
    "item_bias_s = inner_train.groupby('i_idx')['rating'].mean()\n",
    "user_bias = user_bias_s.reindex(range(n_users)).fillna(0).values \n",
    "item_bias = item_bias_s.reindex(range(n_items)).fillna(0).values \n",
    "\n",
    "# Center Matrix\n",
    "print(\"Constructing Centered Matrix...\")\n",
    "R_centered = np.zeros((n_users, n_items), dtype=float)\n",
    "for row in inner_train.itertuples(index=False):\n",
    "    u, i = int(row.u_idx), int(row.i_idx)\n",
    "    R_centered[u, i] = row.rating - user_bias[u] - item_bias[i]\n",
    "\n",
    "# Tuning Function\n",
    "def train_svd_and_eval(k, R_centered, val_df, user_bias, item_bias):\n",
    "    svd = TruncatedSVD(n_components=k, random_state=42)\n",
    "    U = svd.fit_transform(R_centered)\n",
    "    V = svd.components_\n",
    "    Z_hat = U @ V \n",
    "\n",
    "    val_preds = []\n",
    "    val_actuals = []\n",
    "    for row in val_df.itertuples(index=False):\n",
    "        u, i = int(row.u_idx), int(row.i_idx)\n",
    "        # Handle cases where user/item might be missing from inner split\n",
    "        if u < Z_hat.shape[0] and i < Z_hat.shape[1]:\n",
    "            pred = user_bias[u] + item_bias[i] + Z_hat[u, i]\n",
    "            val_preds.append(pred)\n",
    "            val_actuals.append(row.rating)\n",
    "            \n",
    "    if not val_preds: return svd, Z_hat, float('inf')\n",
    "    return svd, Z_hat, mean_absolute_error(val_actuals, val_preds)\n",
    "\n",
    "# Run Tuning\n",
    "k_list = [5, 10, 20, 30] \n",
    "best_k = None\n",
    "best_mae = float('inf')\n",
    "best_Z_hat = None\n",
    "\n",
    "print(f\"Tuning k over: {k_list}\")\n",
    "for k in k_list:\n",
    "    svd, Z_hat, mae_val = train_svd_and_eval(k, R_centered, inner_val, user_bias, item_bias)\n",
    "    print(f\"  k={k}: MAE = {mae_val:.4f}\")\n",
    "    if mae_val < best_mae:\n",
    "        best_mae = mae_val\n",
    "        best_k = k\n",
    "        best_Z_hat = Z_hat\n",
    "\n",
    "print(f\"âœ“ Best model: k={best_k}\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. STAGE 3: SIMULATION (ON TEST SET)\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING SIMULATION ON REPEAT CUSTOMERS TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "test_orders = global_test_df.groupby('order_id')\n",
    "counter = 0\n",
    "\n",
    "for order_id, group in test_orders:\n",
    "    counter += 1\n",
    "    \n",
    "    # Extract Context\n",
    "    price_total = group['price'].sum()\n",
    "    freight_total = group['freight_value'].sum()\n",
    "    days_actual = group['delivery_days'].max()\n",
    "    # IMPORTANT: Use unique ID here too\n",
    "    customer_unique_id = group['customer_unique_id'].iloc[0] \n",
    "    actual_satisfaction = group['high_satisfaction'].max()\n",
    "\n",
    "    # Predict Satisfaction\n",
    "    features = pd.DataFrame([[price_total, freight_total, days_actual]], \n",
    "                            columns=['price', 'freight_value', 'delivery_days'])\n",
    "    pred_satisfied = clf.predict(features)[0]\n",
    "\n",
    "    decision = \"\"\n",
    "    recs = []\n",
    "    \n",
    "    if pred_satisfied == 1:\n",
    "        decision = \"UPSELL\"\n",
    "        \n",
    "        # Check if customer_unique_id exists in our training user map\n",
    "        if customer_unique_id in user2idx:\n",
    "            u_idx = user2idx[customer_unique_id]\n",
    "            scores = user_bias[u_idx] + item_bias + best_Z_hat[u_idx, :]\n",
    "            top_indices = np.argsort(scores)[::-1][:3]\n",
    "            recs = [idx2item[i] for i in top_indices]\n",
    "        else:\n",
    "            # Only happens if ALL of this user's orders ended up in Test (unlikely for heavy users)\n",
    "            decision = \"UPSELL (New Profile)\" \n",
    "            recs = [\"Trending Items\"]\n",
    "    else:\n",
    "        decision = \"RETENTION\"\n",
    "        recs = [\"Discount Coupon\"]\n",
    "\n",
    "    results.append({\n",
    "        'order_id': order_id,\n",
    "        'customer_id': customer_unique_id, # Log the unique ID\n",
    "        'actual': actual_satisfaction,\n",
    "        'predicted': pred_satisfied,\n",
    "        'action': decision,\n",
    "        'recommendations': recs\n",
    "    })\n",
    "    \n",
    "    if counter >= 1000: break\n",
    "\n",
    "# ==========================================\n",
    "# 6. RESULTS\n",
    "# ==========================================\n",
    "res_df = pd.DataFrame(results)\n",
    "\n",
    "res_df.to_csv('customer_retent.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
